# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17gCQABSi_Pl4b_Ms-_e7RoaZjU3ZA5hY
"""

# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import re

# --- Core Logic Functions ---
# Levenshtein distance function (no changes needed)
def levenshtein_distance(word1, word2):
    len1, len2 = len(word1), len(word2)
    matrix = [[0] * (len2 + 1) for _ in range(len1 + 1)]
    for i in range(len1 + 1):
        matrix[i][0] = i
    for j in range(len2 + 1):
        matrix[0][j] = j
    for i in range(1, len1 + 1):
        for j in range(1, len2 + 1):
            cost = 0 if word1[i-1] == word2[j-1] else 1
            matrix[i][j] = min(
                matrix[i-1][j] + 1,      # deletion
                matrix[i][j-1] + 1,      # insertion
                matrix[i-1][j-1] + cost  # substitution
            )
    return matrix[len1][len2]

# --- NEW DATA LOADING FUNCTION ---
# This is the key change. This function is fast because it loads small, ready-to-use files.
@st.cache_data
def load_models():
    """Loads the pre-processed dictionary and bigram models from CSV files."""
    # Load the files, ensuring all data is treated as strings
    word_dict_df = pd.read_csv("word_dictionary.csv", dtype={'word': str})
    bigram_counts = pd.read_csv("bigram_model.csv", dtype={'word1': str, 'word2': str})

    # --- THIS IS THE FIX ---
    # Remove any rows that might have missing values (which can cause errors)
    word_dict_df.dropna(subset=['word'], inplace=True)
    bigram_counts.dropna(subset=['word1', 'word2'], inplace=True)
    
    return word_dict_df, bigram_counts

# --- Spell Checker Class (UPGRADED) ---
class SpellChecker:
    def __init__(self, word_dict_df, bigram_model_df):
        self.word_dict = word_dict_df
        self.bigram_model = bigram_model_df # <-- NEW: The class now knows about bigrams
        self.vocab = set(word_dict_df['word'].tolist())

    def generate_candidates(self, word, max_distance=2):
        # This function stays the same as before
        candidates = []
        dict_words = self.word_dict['word'].tolist()
        for dict_word in dict_words:
            distance = levenshtein_distance(word, dict_word)
            if 0 < distance <= max_distance:
                frequency = self.word_dict[self.word_dict['word'] == dict_word]['frequency'].iloc[0]
                candidates.append({
                    'word': dict_word,
                    'distance': distance,
                    'frequency': frequency
                })
        candidates.sort(key=lambda x: (x['distance'], -x['frequency']))
        return candidates[:10] # Generate a larger pool of candidates to rank

    # --- NEW METHOD TO ADD CONTEXT ---
    def rank_candidates_by_context(self, candidates, previous_word):
        """Re-ranks candidates based on their probability of following the previous word."""
        if not previous_word or not candidates:
            return [c['word'] for c in candidates[:5]] # Return top 5 if no context

        ranked_suggestions = []
        # Get the total count of the previous word to calculate probability
        prev_word_count = self.word_dict[self.word_dict['word'] == previous_word]['frequency'].iloc[0] if previous_word in self.vocab else 0

        for candidate in candidates:
            candidate_word = candidate['word']
            score = 0
            if prev_word_count > 0:
                # Find the count of the bigram (previous_word, candidate_word)
                bigram_row = self.bigram_model[
                    (self.bigram_model['word1'] == previous_word) & 
                    (self.bigram_model['word2'] == candidate_word)
                ]
                
                if not bigram_row.empty:
                    bigram_count = bigram_row['frequency'].iloc[0]
                    # Simple scoring: probability = count(bigram) / count(word1)
                    score = bigram_count / prev_word_count

            ranked_suggestions.append({'word': candidate_word, 'score': score})

        # Sort by the context score (highest first), then return the top 5 words
        ranked_suggestions.sort(key=lambda x: x['score'], reverse=True)
        return [s['word'] for s in ranked_suggestions[:5]]

    def check_non_word_errors(self, words):
        # This function also stays the same
        errors = []
        for i, word in enumerate(words):
            cleaned_word = re.sub(r'[^\w\s]', '', word).lower()
            if cleaned_word and cleaned_word not in self.vocab:
                # We now pass the original word and its cleaned version
                errors.append({
                    'position': i,
                    'original_word': word,
                    'cleaned_word': cleaned_word
                })
        return errors

# --- STREAMLIT USER INTERFACE ---

# Set up the page title and icon
st.set_page_config(page_title="Medical Spell Checker", page_icon="ðŸ©º")

st.title("ðŸ©º Medical Text Spell Checker")
st.write("This tool checks for spelling errors in medical text. It uses a dictionary built from the PubMed 20k RCT dataset to find and suggest corrections.")

# Load the data using our NEW cached function
with st.spinner("Loading dictionary and models..."):
    word_dictionary, bigram_model = load_models()

# Create an instance of our spell checker
spell_checker = SpellChecker(word_dictionary, bigram_model)

st.success("Models loaded successfully! Ready to check your text.")

# Create the text area for user input
user_text = st.text_area("Enter text to check:", "The pateint has diabetis and needs treatmnt.", height=150)

# Create a button to trigger the spell check
if st.button("Check Spelling"):
    if user_text:
        words = re.findall(r'\b\w+\b|[.,;?!]', user_text)
        
        non_word_errors = spell_checker.check_non_word_errors(words)
        
        st.subheader("Results:")
        
        if not non_word_errors:
            st.success("âœ… No spelling errors detected!")
        else:
            st.warning(f"Found {len(non_word_errors)} potential spelling error(s).")
            
            for error in non_word_errors:
                # First, generate candidates based on spelling
                candidates = spell_checker.generate_candidates(error['cleaned_word'])
                
                # --- THIS IS THE UPGRADE ---
                # Get the previous word for context (if it exists)
                previous_word = words[error['position'] - 1].lower() if error['position'] > 0 else None
                
                # Now, re-rank the candidates using the new context method
                final_suggestions = spell_checker.rank_candidates_by_context(candidates, previous_word)
                
                with st.container(border=True):
                    st.metric(label="Misspelled Word", value=error['original_word'])
                    suggestions_text = ", ".join(final_suggestions) if final_suggestions else "No suggestions found"
                    st.info(f"**Suggestions:** {suggestions_text}")
                    
            # --- ADD THIS NEW CODE BLOCK ---

            # Create a corrected version of the text
            st.subheader("Corrected Text:")
            corrected_words = words[:] # Create a copy of the original word list
            
            # Loop through the errors in reverse to avoid index issues while replacing
            for error in reversed(non_word_errors):
                # Generate candidates and rank them to find the best one
                candidates = spell_checker.generate_candidates(error['cleaned_word'])
                previous_word = words[error['position'] - 1].lower() if error['position'] > 0 else None
                final_suggestions = spell_checker.rank_candidates_by_context(candidates, previous_word)
                
                # Replace the misspelled word with the top suggestion
                if final_suggestions:
                    corrected_words[error['position']] = final_suggestions[0]

            # Join the corrected words back into a sentence
            corrected_sentence = " ".join(corrected_words)
            # A bit of regex to clean up spacing around punctuation
            corrected_sentence = re.sub(r'\s+([.,;?!])', r'\1', corrected_sentence)
            
            st.success(corrected_sentence)
    else:
        st.warning("Please enter some text to check.")

st.sidebar.header("About")
st.sidebar.info("This spell checker uses a custom dictionary built from medical abstracts. The core logic is based on Levenshtein distance for generating candidate corrections for non-word errors.")
