# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17gCQABSi_Pl4b_Ms-_e7RoaZjU3ZA5hY
"""

# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import re

# --- Core Logic Functions ---
# Levenshtein distance function (no changes needed)
def levenshtein_distance(word1, word2):
    len1, len2 = len(word1), len(word2)
    matrix = [[0] * (len2 + 1) for _ in range(len1 + 1)]
    for i in range(len1 + 1):
        matrix[i][0] = i
    for j in range(len2 + 1):
        matrix[0][j] = j
    for i in range(1, len1 + 1):
        for j in range(1, len2 + 1):
            cost = 0 if word1[i-1] == word2[j-1] else 1
            matrix[i][j] = min(
                matrix[i-1][j] + 1,      # deletion
                matrix[i][j-1] + 1,      # insertion
                matrix[i-1][j-1] + cost  # substitution
            )
    return matrix[len1][len2]

# --- NEW DATA LOADING FUNCTION ---
# This is the key change. This function is fast because it loads small, ready-to-use files.
@st.cache_data
def load_models():
    """Loads the pre-processed dictionary and bigram models from CSV files."""
    word_dict_df = pd.read_csv("word_dictionary.csv")
    bigram_counts = pd.read_csv("bigram_model.csv")
    return word_dict_df, bigram_counts

# --- Spell Checker Class ---
class SpellChecker:
    def __init__(self, word_dict_df):
        self.word_dict = word_dict_df
        self.vocab = set(word_dict_df['word'].tolist())

    def generate_candidates(self, word, max_distance=2):
        candidates = []
        dict_words = self.word_dict['word'].tolist()
        for dict_word in dict_words:
            distance = levenshtein_distance(word, dict_word)
            if 0 < distance <= max_distance:
                frequency = self.word_dict[self.word_dict['word'] == dict_word]['frequency'].iloc[0]
                candidates.append({
                    'word': dict_word,
                    'distance': distance,
                    'frequency': frequency
                })
        candidates.sort(key=lambda x: (x['distance'], -x['frequency']))
        return candidates[:5] # Return top 5

    def check_non_word_errors(self, words):
        errors = []
        for i, word in enumerate(words):
            # Simple word cleaning for checking
            cleaned_word = re.sub(r'[^\w\s]', '', word).lower()
            if cleaned_word and cleaned_word not in self.vocab:
                suggestions = self.generate_candidates(cleaned_word)
                errors.append({
                    'position': i,
                    'word': word, # Keep the original word for display
                    'suggestions': [s['word'] for s in suggestions]
                })
        return errors

# --- STREAMLIT USER INTERFACE ---

# Set up the page title and icon
st.set_page_config(page_title="Medical Spell Checker", page_icon="ðŸ©º")

st.title("ðŸ©º Medical Text Spell Checker")
st.write("This tool checks for spelling errors in medical text. It uses a dictionary built from the PubMed 20k RCT dataset to find and suggest corrections.")

# Load the data using our NEW cached function
with st.spinner("Loading dictionary and models..."):
    word_dictionary, bigram_model = load_models()

# Create an instance of our spell checker
spell_checker = SpellChecker(word_dictionary)

st.success("Models loaded successfully! Ready to check your text.")

# Create the text area for user input
user_text = st.text_area("Enter text to check:", "The pateint has diabetis and needs treatmnt.", height=150)

# Create a button to trigger the spell check
if st.button("Check Spelling"):
    if user_text:
        # Split text into words while preserving punctuation
        words = re.findall(r'\b\w+\b|[.,;?!]', user_text)

        # Perform the check for non-word errors
        non_word_errors = spell_checker.check_non_word_errors(words)

        st.subheader("Results:")

        if not non_word_errors:
            st.success("âœ… No spelling errors detected!")
        else:
            st.warning(f"Found {len(non_word_errors)} potential spelling error(s).")

            # Display errors and suggestions in a structured way
            for error in non_word_errors:
                with st.container(border=True):
                    st.metric(label="Misspelled Word", value=error['word'])
                    suggestions = ", ".join(error['suggestions']) if error['suggestions'] else "No suggestions found"
                    st.info(f"**Suggestions:** {suggestions}")
    else:
        st.warning("Please enter some text to check.")

st.sidebar.header("About")
st.sidebar.info("This spell checker uses a custom dictionary built from medical abstracts. The core logic is based on Levenshtein distance for generating candidate corrections for non-word errors.")