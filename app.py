# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17gCQABSi_Pl4b_Ms-_e7RoaZjU3ZA5hY
"""

# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import re

# --- Core Logic Functions ---
def levenshtein_distance(word1, word2):
    len1, len2 = len(word1), len(word2)
    matrix = [[0] * (len2 + 1) for _ in range(len1 + 1)]
    for i in range(len1 + 1):
        matrix[i][0] = i
    for j in range(len2 + 1):
        matrix[0][j] = j
    for i in range(1, len1 + 1):
        for j in range(1, len2 + 1):
            cost = 0 if word1[i-1] == word2[j-1] else 1
            matrix[i][j] = min(
                matrix[i-1][j] + 1,
                matrix[i][j-1] + 1,
                matrix[i-1][j-1] + cost
            )
    return matrix[len1][len2]

# --- Data Loading Function ---
@st.cache_data
def load_models():
    """Loads the pre-processed dictionary and bigram models from CSV files."""
    word_dict_df = pd.read_csv("word_dictionary.csv", dtype={'word': str})
    bigram_counts = pd.read_csv("bigram_model.csv", dtype={'word1': str, 'word2': str})
    word_dict_df.dropna(subset=['word'], inplace=True)
    bigram_counts.dropna(subset=['word1', 'word2'], inplace=True)
    return word_dict_df, bigram_counts

# --- Spell Checker Class ---
class SpellChecker:
    def __init__(self, word_dict_df, bigram_model_df):
        self.word_dict = word_dict_df
        self.bigram_model = bigram_model_df
        self.vocab = set(word_dict_df['word'].tolist())

    def generate_candidates(self, word, max_distance=2):
        candidates = []
        dict_words = self.word_dict['word'].tolist()
        for dict_word in dict_words:
            distance = levenshtein_distance(word, dict_word)
            if 0 < distance <= max_distance:
                frequency = self.word_dict[self.word_dict['word'] == dict_word]['frequency'].iloc[0]
                candidates.append({
                    'word': dict_word,
                    'distance': distance,
                    'frequency': frequency
                })
        candidates.sort(key=lambda x: (x['distance'], -x['frequency']))
        return candidates[:10]

    def rank_candidates_by_context(self, candidates, previous_word):
        """Re-ranks candidates based on their probability of following the previous word."""
        if not previous_word or not candidates:
            return [c['word'] for c in candidates[:5]]

        ranked_suggestions = []
        prev_word_count = self.word_dict[self.word_dict['word'] == previous_word]['frequency'].iloc[0] if previous_word in self.vocab else 0

        for candidate in candidates:
            candidate_word = candidate['word']
            score = 0
            if prev_word_count > 0:
                bigram_row = self.bigram_model[
                    (self.bigram_model['word1'] == previous_word) &
                    (self.bigram_model['word2'] == candidate_word)
                ]
                if not bigram_row.empty:
                    bigram_count = bigram_row['frequency'].iloc[0]
                    score = bigram_count / prev_word_count
            ranked_suggestions.append({'word': candidate_word, 'score': score})

        ranked_suggestions.sort(key=lambda x: x['score'], reverse=True)
        return [s['word'] for s in ranked_suggestions[:5]]

    def check_non_word_errors(self, words):
        errors = []
        for i, word in enumerate(words):
            cleaned_word = re.sub(r'[^\w\s]', '', word).lower()
            if cleaned_word and cleaned_word not in self.vocab:
                errors.append({
                    'position': i,
                    'original_word': word,
                    'cleaned_word': cleaned_word
                })
        return errors

# --- STREAMLIT PAGE CONFIG AND TITLE ---
st.set_page_config(page_title="Medical Spell Checker", page_icon="ðŸ©º")
st.title("ðŸ©º Medical Text Spell Checker")
st.write("This tool checks for spelling errors in medical text. It uses a dictionary built from the PubMed 20k RCT dataset to find and suggest corrections.")

# --- LOAD MODELS ---
with st.spinner("Loading dictionary and models..."):
    word_dictionary, bigram_model = load_models()
spell_checker = SpellChecker(word_dictionary, bigram_model)
st.success("Models loaded successfully! Ready to check your text.")

# --- USER INPUT AREA ---
user_text = st.text_area("Enter text to check:", "The pateint has diabetis and needs treatmnt.", height=150)

# --- UI HELPER FUNCTIONS ---
def update_word(error_index, new_word):
    error_position = st.session_state.errors[error_index]['position']
    st.session_state.words[error_position] = new_word
    st.session_state.errors[error_index]['resolved'] = True

def ignore_error(error_index):
    st.session_state.errors[error_index]['resolved'] = True

# --- INTERACTIVE UI BLOCK ---
if st.button("Check Spelling", key="main_check_button"):
    if user_text:
        st.session_state.words = re.findall(r'\b\w+\b|[.,;?!]', user_text)
        errors = spell_checker.check_non_word_errors(st.session_state.words)
        for error in errors:
            error['resolved'] = False
        st.session_state.errors = errors
    else:
        st.warning("Please enter some text to check.")

if 'errors' in st.session_state and st.session_state.errors:
    st.subheader("Interactive Corrections:")
    unresolved_errors = [e for e in st.session_state.errors if not e.get('resolved', False)]
    if not unresolved_errors:
        st.success("All errors have been resolved!")
    else:
        st.warning(f"Found {len(unresolved_errors)} unresolved spelling error(s).")

    for i, error in enumerate(st.session_state.errors):
        if not error.get('resolved', False):
            with st.container(border=True):
                candidates = spell_checker.generate_candidates(error['cleaned_word'])
                previous_word = st.session_state.words[error['position'] - 1].lower() if error['position'] > 0 else None
                final_suggestions = spell_checker.rank_candidates_by_context(candidates, previous_word)
                col1, col2 = st.columns([1, 3])
                with col1:
                    st.error(f"**{error['original_word']}**")
                with col2:
                    top_suggestions = final_suggestions[:3]
                    action_cols = st.columns(len(top_suggestions) + 1)
                    for idx, suggestion in enumerate(top_suggestions):
                        with action_cols[idx]:
                            st.button(
                                suggestion,
                                key=f"sugg_{i}_{suggestion}",
                                on_click=update_word,
                                args=(i, suggestion),
                                use_container_width=True
                            )
                    with action_cols[-1]:
                        st.button(
                            "Ignore",
                            key=f"ignore_{i}",
                            on_click=ignore_error,
                            args=(i,),
                            use_container_width=True
                        )

    st.subheader("Corrected Text:")
    corrected_sentence = " ".join(st.session_state.words)
    corrected_sentence = re.sub(r'\s+([.,;?!])', r'\1', corrected_sentence)
    st.info(corrected_sentence)

# --- SIDEBAR ---
st.sidebar.header("About")
st.sidebar.info("This spell checker uses a custom dictionary built from medical abstracts. The core logic is based on Levenshtein distance for generating candidate corrections for non-word errors.")
